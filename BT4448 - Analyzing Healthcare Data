# importing important libraries
import numpy as np # for working with arrays
import pandas as pd # for data analytics
import matplotlib as plt # for data visualization
import seaborn as sns # for advanced data visualization 
import matplotlib.pyplot as plt

print("(Rows, columns): " + str(data.shape))
data.columns 

data.nunique(axis=0) # number of unique values in each column

data.describe() # description of dataset

print(data.isna().sum()) # getting number of null values in each column

data['target'].value_counts() # our dataset has comparable number of positive and negative heart disease patients

# correlation matrix
corr = data.corr()
plt.subplots(figsize=(15,10))
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))
sns.heatmap(corr, xticklabels=corr.columns,
            yticklabels=corr.columns, 
            annot=True,
            cmap=sns.diverging_palette(220, 20, as_cmap=True))

# Pairplot to plot multiple pairwise bivariate distributions in a dataset
subData = data[['age','trestbps','chol','thalach','oldpeak']]
sns.pairplot(subData)

# catplot() method is used to plot categorical plots
sns.catplot(x="target", y="oldpeak", hue="slope", kind="bar", data=data);

plt.title('ST depression (induced by exercise relative to rest) vs. Heart Disease',size=25)
plt.xlabel('Heart Disease',size=20)
plt.ylabel('ST depression',size=20)

# ViolinPlot shows several quantitative data across one or more categorical variables.
plt.figure(figsize=(12,8))
sns.violinplot(x= 'target', y= 'oldpeak',hue="sex", inner='quartile',data= data )
plt.title("Thalach Level vs. Heart Disease",fontsize=20)
plt.xlabel("Heart Disease Target", fontsize=16)
plt.ylabel("Thalach Level", fontsize=16)

# A Box Plot is created to display the summary of the set of data values having properties like minimum, first quartile, median, third quartile and maximum. 
plt.figure(figsize=(12,8))
sns.boxplot(x= 'target', y= 'thalach',hue="sex", data=data )
plt.title("ST depression Level vs. Heart Disease", fontsize=20)
plt.xlabel("Heart Disease Target",fontsize=16)
plt.ylabel("ST depression induced by exercise relative to rest", fontsize=16)

# Filtering data by positive Heart Disease patient 
pos_data = data[data['target']==1]
pos_data.describe()

# Filtering data by negative Heart Disease patient 
neg_data = data[data['target']==0]
neg_data.describe()

print("(Positive Patients ST depression): " + str(pos_data['oldpeak'].mean()))
print("(Negative Patients ST depression): " + str(neg_data['oldpeak'].mean()))

print("(Positive Patients thalach): " + str(pos_data['thalach'].mean()))
print("(Negative Patients thalach): " + str(neg_data['thalach'].mean()))

# Assigning the 13 features to X, & the last column to our classification predictor, y
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

# splitting the data set into the Training set and Test set
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 1)  

#  Standardizing the data will transform the data so that its distribution will have a mean of 0 and a standard deviation of 1.
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

from sklearn.metrics import classification_report 
from sklearn.linear_model import LogisticRegression

model1 = LogisticRegression(random_state=1) # get instance of model
model1.fit(x_train, y_train) # Train/Fit model 

y_pred1 = model1.predict(x_test) # get y predictions
print(classification_report(y_test, y_pred1)) # output accuracy

from sklearn.metrics import classification_report 
from sklearn.neighbors import KNeighborsClassifier

model2 = KNeighborsClassifier() 
model2.fit(x_train, y_train) 

y_pred2 = model2.predict(x_test) 
print(classification_report(y_test, y_pred2)) 

from sklearn.metrics import classification_report 
from sklearn.svm import SVC

model3 = SVC(random_state=1) 
model3.fit(x_train, y_train) 

y_pred3 = model3.predict(x_test) 
print(classification_report(y_test, y_pred3)) 

from sklearn.metrics import classification_report 
from sklearn.naive_bayes import GaussianNB

model4 = GaussianNB() 
model4.fit(x_train, y_train) 

y_pred4 = model4.predict(x_test) 
print(classification_report(y_test, y_pred4)) 

from sklearn.metrics import classification_report 
from sklearn.tree import DecisionTreeClassifier

model5 = DecisionTreeClassifier(random_state=1) 
model5.fit(x_train, y_train) 

y_pred5 = model5.predict(x_test) 
print(classification_report(y_test, y_pred5)) 

from sklearn.metrics import classification_report 
from sklearn.ensemble import RandomForestClassifier

model6 = RandomForestClassifier(random_state=1)
model6.fit(x_train, y_train)  

y_pred6 = model6.predict(x_test) 
print(classification_report(y_test, y_pred6)) 

# A confusion matrix is a table that is used to define the performance of a classification algorithm.
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred6)
print(cm)
accuracy_score(y_test, y_pred6)

# Feature Importance provides a score that indicates how helpful each feature was in our model.
# The higher the Feature Score, the more that feature is used to make key decisions & thus the more important it is.

# get importance
importance = model6.feature_importances_

# summarize feature importance
for i,v in enumerate(importance):
    print('Feature: %0d, Score: %.5f' % (i,v))
index= data.columns[:-1]
importance = pd.Series(model6.feature_importances_, index=index)
importance.nlargest(13).plot(kind='barh', colormap='winter')

# First value represents our predicted value, Second value represents our actual value.
# If the values match, then we predicted correctly.

y_pred = model6.predict(x_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
